{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDv6ENEkUGSZ6Lr0EzI3LR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amzad-786githumb/AI_and_ML_by-Microsoft/blob/main/16_Implementing_feature_selection_techniques_on_a_given_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Tasks:</h2>\n",
        "\n",
        "\n",
        "*   **Implement cross-validation:** Apply cross-validation techniques to evaluate the robustness of supervised learning models.\n",
        "*   **Use key evaluation metrics:** Calculate and interpret metrics such as accuracy, precision, recall, F1-score, and R-squared for model assessment.\n",
        "*   **Improve model reliability:** Ensure that model performance is generalizable by using cross-validation combined with multiple evaluation metrics.\n"
      ],
      "metadata": {
        "id": "gJuJdlMRboiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>1. Setting up your environment</h3>"
      ],
      "metadata": {
        "id": "fwuAUpi2H3Fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy statsmodels scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHh1togaH71U",
        "outputId": "91806603-3367-4bc7-97f4-829dc4aa894a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (0.14.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.16.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (25.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>2: Importing the required libraries</h3>"
      ],
      "metadata": {
        "id": "Lwmm4u3LcHf9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cd879a1"
      },
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>3: Load and prepare the data</h3>"
      ],
      "metadata": {
        "id": "jiCY5Mz6kxJO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65b2a02d"
      },
      "source": [
        "# Sample dataset: Study hours, previous exam scores, and pass/fail labels\n",
        "data = {\n",
        "    'StudyHours': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'PrevExamScore': [30, 40, 45, 50, 60, 65, 70, 75, 80, 85],\n",
        "    'Pass': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]  # 0 = Fail, 1 = Pass\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Features and target variable\n",
        "X = df[['StudyHours', 'PrevExamScore']]  # Features\n",
        "y = df['Pass']  # Target variable"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>4. Implementing backward elimination</h3>"
      ],
      "metadata": {
        "id": "u92rturEByml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backward elimination starts with all features and removes those that are not statistically significant based on their p-values.\n",
        "\n",
        "**Steps for backward elimination**\n",
        "\n",
        "1.  Add a constant (intercept) to the feature set.\n",
        "\n",
        "2.  Fit the model and check p-values.\n",
        "\n",
        "3.  Remove the feature with the highest p-value greater than 0.05.\n",
        "\n",
        "4.  Repeat until all remaining features have p-values below 0.05."
      ],
      "metadata": {
        "id": "hfxLgBNPIyLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#add a constant to a model\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "#Fit the model using the OLS\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "#Model summary\n",
        "print(model.summary())\n",
        "\n",
        "#remove features with highest p value greater than 0.5\n",
        "if model.pvalues['StudyHours'] > 0.5:\n",
        "  X = X.drop(columns='StudyHours', axis=1)\n",
        "  model = sm.OLS(y, X).fit()\n",
        "\n",
        "#final model after backward elimination\n",
        "print(\"****************************************************************\")\n",
        "print(\"Model summary after backward elimination\")\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "jlg_n5knB3n1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab63dd6d-cde6-4496-c9be-66d576149944"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                   Pass   R-squared:                       0.758\n",
            "Model:                            OLS   Adj. R-squared:                  0.688\n",
            "Method:                 Least Squares   F-statistic:                     10.94\n",
            "Date:                Thu, 09 Oct 2025   Prob (F-statistic):            0.00701\n",
            "Time:                        04:10:27   Log-Likelihood:               -0.17258\n",
            "No. Observations:                  10   AIC:                             6.345\n",
            "Df Residuals:                       7   BIC:                             7.253\n",
            "Df Model:                           2                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=================================================================================\n",
            "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
            "---------------------------------------------------------------------------------\n",
            "const            -0.3333      1.464     -0.228      0.826      -3.796       3.129\n",
            "StudyHours        0.1515      0.324      0.468      0.654      -0.615       0.918\n",
            "PrevExamScore -3.053e-16      0.054  -5.68e-15      1.000      -0.127       0.127\n",
            "==============================================================================\n",
            "Omnibus:                        0.086   Durbin-Watson:                   1.491\n",
            "Prob(Omnibus):                  0.958   Jarque-Bera (JB):                0.311\n",
            "Skew:                           0.000   Prob(JB):                        0.856\n",
            "Kurtosis:                       2.136   Cond. No.                     1.01e+03\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 1.01e+03. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n",
            "****************************************************************\n",
            "Model summary after backward elimination\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                   Pass   R-squared:                       0.750\n",
            "Model:                            OLS   Adj. R-squared:                  0.719\n",
            "Method:                 Least Squares   F-statistic:                     24.00\n",
            "Date:                Thu, 09 Oct 2025   Prob (F-statistic):            0.00120\n",
            "Time:                        04:10:27   Log-Likelihood:               -0.32644\n",
            "No. Observations:                  10   AIC:                             4.653\n",
            "Df Residuals:                       8   BIC:                             5.258\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=================================================================================\n",
            "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
            "---------------------------------------------------------------------------------\n",
            "const            -1.0000      0.319     -3.138      0.014      -1.735      -0.265\n",
            "PrevExamScore     0.0250      0.005      4.899      0.001       0.013       0.037\n",
            "==============================================================================\n",
            "Omnibus:                        0.471   Durbin-Watson:                   1.575\n",
            "Prob(Omnibus):                  0.790   Jarque-Bera (JB):                0.372\n",
            "Skew:                          -0.375   Prob(JB):                        0.830\n",
            "Kurtosis:                       2.425   Cond. No.                         225.\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>5. Implementing forward selection</h3>"
      ],
      "metadata": {
        "id": "f0VZK-7dKiT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward selection adds features one at a time based on their contribution to the model’s performance.\n",
        "\n",
        "**Steps for forward selection**\n",
        "\n",
        "1.  Start with an empty model.\n",
        "\n",
        "2.  Add one feature at a time that improves the model’s performance.\n",
        "\n",
        "3.  Stop when adding features no longer improves the model."
      ],
      "metadata": {
        "id": "A1D83PxAKmtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_selection(X, y):\n",
        "    remaining_features = set(X.columns)\n",
        "    selected_features = []\n",
        "    current_score = 0.0\n",
        "\n",
        "    while remaining_features:\n",
        "        scores_with_candidates = []\n",
        "\n",
        "        for feature in remaining_features:\n",
        "            features_to_test = selected_features + [feature]\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X[features_to_test], y, test_size=0.2, random_state=42)\n",
        "\n",
        "            # Train the model\n",
        "            model = LinearRegression()\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            score = r2_score(y_test, y_pred)\n",
        "\n",
        "            scores_with_candidates.append((score, feature))\n",
        "\n",
        "        # Select the feature with the highest score\n",
        "        scores_with_candidates.sort(reverse=True)\n",
        "        best_score, best_feature = scores_with_candidates[0]\n",
        "\n",
        "        if current_score < best_score:\n",
        "            remaining_features.remove(best_feature)\n",
        "            selected_features.append(best_feature)\n",
        "            current_score = best_score\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return selected_features\n",
        "\n",
        "best_features = forward_selection(X, y)\n",
        "print(f\"Selected features using Forward Selection: {best_features}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIW71OvGKxaa",
        "outputId": "733e586f-6249-4c75-f2f6-5f3f9bac375b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features using Forward Selection: ['PrevExamScore']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>6. Implementing LASSO</h3>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EakkThDNgTEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LASSO is a regularization technique that automatically shrinks the coefficients of less important features to zero, effectively performing feature selection.\n",
        "\n",
        "**Steps for LASSO**\n",
        "\n",
        "1.  Initialize the LASSO model with a regularization parameter.\n",
        "\n",
        "2.  Fit the model on the training data.\n",
        "\n",
        "3.  Analyze which features have nonzero coefficients."
      ],
      "metadata": {
        "id": "qwFk3Dz4miVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the LASSO model with alpha (regularization parameter)\n",
        "lasso_model = Lasso(alpha=0.1)\n",
        "\n",
        "# Train the LASSO model\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = lasso_model.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f'R-squared score: {r2}')\n",
        "\n",
        "# Display the coefficients of the features\n",
        "print(f'LASSO Coefficients: {lasso_model.coef_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91CJr4tXijXO",
        "outputId": "8dd5114c-0b23-4dec-96d0-1323f6a2013b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared score: 0.9997884297520662\n",
            "LASSO Coefficients: [0.         0.02463636]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>6. Analyzing the results</h3>"
      ],
      "metadata": {
        "id": "n9PRVOsCwmeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the coefficients of the features\n",
        "print(f'LASSO Coefficients: {lasso_model.coef_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymg1-TnENpOD",
        "outputId": "30e41f33-6c9e-425e-b3b2-a5274037e481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LASSO Coefficients: [0.         0.02463636]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The coefficient for StudyHours is 0, meaning it was removed from the model.\n",
        "*   The coefficient for PrevExamScore is nonzero, meaning it was retained in the model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KRmEp2egNwBq"
      }
    }
  ]
}