{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ_MoprcR0MM"
      },
      "source": [
        "<h2>Tasks: </h2>\n",
        "* Efficiently pre-process your own data, ensuring it's ready for training and optimized for success in real-world tasks.\n",
        "* Prepare a dataset for fine-tuning.\n",
        "* Clean the raw data, tokenize the text, handle missing data, and structure it into a training-ready input for a fine-tuning task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-LHRwGASqyJ"
      },
      "source": [
        "<h3>Step 1: Import dataset</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbZsYR_LWOT3"
      },
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
        "data = pd.read_csv('hf://datasets/stepp1/tweet_emotion_intensity/' + splits['train'])\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9mYwkBRUrnd"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IU_KNQ1WvaX"
      },
      "source": [
        "<h3>Step 2: Clean the text</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F0BBwdwVU9A"
      },
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "data['cleaned_text'] = data['tweet'].apply(clean_text)\n",
        "print(data['cleaned_text'].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXfm3uHMXQ42"
      },
      "source": [
        "<h3>Step 3: Handle missing data</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tiUKdrAXSI2"
      },
      "source": [
        "print(data.isnull().sum())\n",
        "data = data.dropna(subset=['cleaned_text'])\n",
        "data['cleaned_text'] = data['cleaned_text'].fillna('unknown')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShRbulz4XiJe"
      },
      "source": [
        "<h3>Step 4: Tokenization</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "token_step"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "tokens = tokenizer(\n",
        "    data['cleaned_text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt'\n",
        ")\n",
        "\n",
        "print(tokens['input_ids'][:5])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
